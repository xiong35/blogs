# 《数学之美》读书笔记(一)

## 语意模型--马尔可夫假设

- 每个词的出现概率只和前一个词有关，根据大数定律，两者共同出现的概率等于语料库中两者一起出现的次数除以语料库大小。
- n阶马尔可夫假设(n元模型）：出现概率与前面n-1个词有关

- 解决语料库中零概率问题的方法：古德－图灵估计
  - 假定r较小时统计可能不可靠，因此出现r次的词不应有那么高的权重
  - 原本的计算概率方式：
    - $N=\displaystyle\sum^{\infty}_{r=1}rN_r$
    - 其中出现r次的词有$Nr$个,N为语料库大小
  - 现在的方式：
    - $d_r=(r+1)\cdot N_{r+1}/N_r$
    - 其中$d_r$是估计的概率

- 局限：
  - 复杂度大
  - 难找到上下文关联

---

## 隐含马尔科夫模型

- 马尔科夫链
  - 一个状态本该和以前所有状态有关，但是马尔科夫链中只考虑上**一个**状态
  - 实质是一个有穷状态机，随机选择一个状态开始，按自身规则随机转移到下一个状态
  - 组成成分：
    - 状态
    - 状态转移概率
    - 结果
    - 产生结果概率（独立输出假设）

- 隐含马尔科夫模型：缺少上述成分中的一个
  - 三个基本问题
    1. 给定模型，计算产生某个特定输出的概率: forward-backward算法
    2. 给定模型和输出序列，找到最可能的状态序列: 维特比算法
    3. 给定足够观测数据，估计模型的各个参数（如下）

  - 需要求解
    1. $P(o_t|s_t) = \frac{P(o_t,s_t)}{P(s_t)}$--------（1）
    2. $P(s_t|s_{t-1}) = \frac{P(s_{t-1},s_t)}{P(s_{t-1})}$--------（2）
其中$s_t$为第t个状态，$o_t$为第t个输出

  - 有监督：如果知道$s_t$出现次数，每次经过这个状态时产生的输出，那么$P(o_t|s_t) = \frac{\#(o_t,s_t)}{\#(s_t)}$

  - 无监督：使用鲍姆-韦尔奇算法：
    1. 找到一组能够产生输出序列O的参数，这个初始模型记为$M_{\theta_0}$
    2. 为了找到更好的模型，我们假定已经解决了问题1，2，可以算出**这个模型产生O的概率**$P(O|M_{\theta_0})$,以及**这个模型产生O的所有途径及其概率**
    3. 这些可能路径可以看作***标记的训练数据***，根据（1）（2）式迭代至收敛

---

## 信息的度量

- 信息熵
![信息熵](https://s1.ax1x.com/2020/05/12/Ytxs8s.jpg)
  - 定义：$H(X)=-\displaystyle\sum_{x\in X}P(x)logP(x)$
  - 冗余度：文件大小（BIT）与文件所含信息熵的差值（汉语冗余度较小）
  - 香农第一定律：**任何编码的长度不小于他的信息熵**
- 信息的作用：消除不确定性
  - 在Y的条件下X的**条件熵**：

$$H(X|Y)=-\displaystyle\sum_{x\in X,y\in Y}P(x,y)logP(x|y)<H(X)$$

> 所以语义分析的二元模型优于一元模型

- 互信息：对**两个随机事件相关性**的度量

$$I(X;Y)=-\displaystyle\sum_{x\in X,y\in Y}log{\frac{P(x,y)}{P(x)\cdot P(y)}}$$
$$I(X;Y)== H(X)-H(X|Y)$$

- 相对熵（交叉熵，Kullback-Leibler Divergence，KL）:衡量两个取值为正数的函数的相似性

$$KL(f(x)||g(x))=\displaystyle\sum_{x\in X}f(x)\cdot log{\frac{f(x)}{g(x)}}$$

- 结论
  1. 函数相差越大，相对熵越大
  2. 对于概率分布/概率密度函数，如果取值大于零，相对熵可以度量两个随机分布的差异

---

## 搜索引擎

- 关键词权重的度量：TF-IDF
  - Term Frequency/Inverse Document Frequency
  - 每个词的重要程度不一样，所以对每一个词设一个权重
  - 逆文本频率指数
    - 思想：一个词在越多网页里出现，他的权重就越小
    - $IDF = log(\frac{D}{D_w})$
      - 其中$D$是全部网页数，$D_w$代表关键词出现在多少个网页里
      - 这里的log是根据交叉熵来的

---

## 新闻分类问题

- 表示新闻
  - 将词汇表中的每一个词进行TF-IDF编号，这个排列当作特征向量
  - 相似性：余弦距离

- 分类
  1. 未知类别向量：自底向上不断合并最相近的，再截出想要的种类数
  2. 已知类别：直接分

- tricks
  - 大数时的余弦计算
    - 计算余弦距离时把长度乘积存起来复用
    - 只计算非零部分
    - 删除虚词
  - 位置加权
    - 标题/文本/段首段尾

---

## 矩阵分解

- 奇异值分解（Singular Value Decomposition）
  - 减小计算量和储存量
  - e.g. : 1000000\*500000的大矩阵A表示有500000个词，1000000篇新闻，他可以分解为1000000\*100的X矩阵（X记录每篇新闻和某个的相关度），100\*100的B矩阵（B记录语义类和文章种类的相关程度），100\*500000的Y矩阵（Y记录文章和主题间的相关度)的积

- 会丢失信息，但是速度快，可以和余弦方法共同使用（先粗分，再精分）

---
