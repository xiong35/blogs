# 周志华《机器学习》笔记(四)

> 关键词: AI, 读书笔记

### 3.4 线性判别分析

> LDA : Linear Disciminant Analysis

给定训练样例集，设法将样例投影到一条直线上，使得同类的投影点尽可能接近，异类投影点尽可能远离。在对新样本进行分析时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别

给定数据集$D=\{(x_i,y_i)\}_{i=1}^m,y_i\in \{0,1\}$，令$X_i, \mu_i,\Sigma_i$分别为第$i$类示例的集合、均值向量、协方差矩阵，若将数据投影到直线$w$上，则两类样本的中心在直线上的投影分别是$w^T\mu_0,w^T\mu_1$；若将所有样本点投影到直线上，两类样本的协方差分别为$w^T\Sigma_0w,w^T\Sigma_1w$？?

欲使同类样本尽可能近，可以让同类样本的协方差尽可能小，即使$w^T\Sigma_0w+w^T\Sigma_1w$尽可能小；欲使异类样本尽可能远离，可以让异类样本中心距离尽可能大，即使$||w^T\mu_0-w^T\mu_1||_2^2$尽可能大。同时考虑两者，可得到最大化目标：

$$
\begin{aligned}
J& = \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w}
\\
&=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}
\end{aligned}
$$

定义“类内散度矩阵”：

$$
\begin{aligned}
S_w&=\Sigma_0+\Sigma_1\\
& = \displaystyle\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\displaystyle\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T
\end{aligned}
$$

以及“类间散度矩阵”：

$$
S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T
$$

则 J 重写为：

$$
J=\frac{w^TS_bw}{w^TS_ww}
$$

这就是 LDA 欲最大化的目标，即$S_b,S_w$的“广义瑞利商”

如何确定$w$呢，注意到$J$的分子分母都是关于$w$的二次项，所以解和$w$的长度无关，仅与方向有关，不失一般性，令$w^TS_ww=1$，则$J$等价于：

$$
\min_w\quad -w^TS_bw\\
s.t.\quad w^TS_ww=1
$$

由拉格朗日乘子法，上式等价于

$$
S_bw=\lambda S_ww\tag{*}
$$

其中$\lambda$是拉格朗日乘子？?  
注意到$S_bw$的方向恒为$\mu_0-\mu_1$，不妨令

$$
S_bw=\lambda(\mu_0-\mu_1)
$$

代入(\*)式得

$$
w=S^{-1}_w(\mu_0-\mu_1)
$$

实践中通常对$S_w$进行奇异值分解

LDA 可以从贝叶斯决策理论的角度阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相同时，LDA 可达最优分类：  
![LDA的贝叶斯证明](https://s1.ax1x.com/2020/05/12/YNFaNV.jpg)  
可以推广到多分类问题，具体见西瓜书 3.4 末尾？?

### 3.5 多分类问题

基本思路：拆解法，将多分类问题拆成若干二分类问题。拆分策略如下：

- OvO（one vs one）
- OvR（one vs rest）
- MvM（many vs many）

OvO, OvR：不赘述，如图  
![OvO,OvR](https://s1.ax1x.com/2020/05/12/YNFl9S.jpgg)

MvM：每次将若干类作为正类，若干其他类作为反类，正反类要有特殊的构造，不能随意选取，一种最常用的技术叫“纠错输出码技术”（Error Correction Output Codes，ECOC）

ECOC 是将编码思想引入类别拆分，并尽可能在解码过程中具有容错性，主要工作分为两步：

- 编码：对 N 个类别做 M”次划分，每次划分都将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集；一共产生 M 个训练集，训练出 M 个分类器
- M 个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这些编码和各个类别的编码进行比较，返回其中距离最小的类别作为预测结果

类别划分通过“编码矩阵”制定，编码矩阵有多种形式，常见的主要有二元码（指定为正反两类）和三元码（额外制定一个停用类）

为什么称为“纠错输出码”呢，因为在测试阶段 ECOC 对分类器的错误有一定容忍和修正能力？?

### 3.6 类别不平衡问题

#### 再放缩

执行对数概率预测时令

$$
\frac{y'}{1-y'}=\frac{y}{1-y}\times \frac{m^-}{m^+}\tag{*}
$$

其中$m^+,m^-$代表正例、反例数目

实际操作时的问题：未必能有效的基于训练数据观测几率来推断真实几率

#### 欠采样

直接去除较多数据中的部分（代表算法 EasyEnsenble，利用集成学习机制，将反例划分为若干集合供不同模型使用）

#### 过采样

增加一些少的例子（不是简单的重复采样，而是对正例进行插值来产生正例，代表算法有 SMOTE）

#### 阀值移动

训练方法不变，预测时调整阀值为(\*)式

---

## 4 决策树

### 4.1 基本流程

递归的进行一系列决策来进行二分类  
遇到以下情况递归返回

- 当前节点包含的样本全部属于同一类别，无需划分
- 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
- 当前节点包含的样本集为空

在第二种情况下，我们把当前节点标记为叶结点，并将其类别设为该结点所包含样本最多的类别

在第三种情况下同样把当前节点设为叶结点，但将其类别设定为其父结点所含样本最多的类别

情景二是利用当前节点的后验分布，情形三则是把父结点的样本分布作为当前节点的先验分布

### 4.2 划分选择

#### 4.2.1 信息增益

假定当前样本集合$D$中第$k$类样本所占比例为$p_k$，则$D$的信息熵定义为

$$
Ent(D)=-\displaystyle\sum_{k=1}^{|y|}p_k\cdot log_2p_k
$$

信息熵越小$D$的纯度越高

假定离散属性$a$有$V$个可能的取值$\{a^1,a^2,...,a^V\}$，若用$a$对样本集进行划分，则会产生$V$个分支结点，其中第$v$个结点包含了$D$中所有在属性$a$上取值为$a^v$的样本
