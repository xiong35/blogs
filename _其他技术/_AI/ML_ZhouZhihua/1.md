# 周志华《机器学习》笔记(一)

> 关键词: AI, 读书笔记

## 1 绪论

略

---

## 2 模型评估与选择

### 2.1 经验误差与过拟合

若可彻底避免过拟合，则通过经验误差（training error）最小化就能获得最优解，这就意味着我们构造性的证明了$P = NP$，因此，只要相信$P\not ={NP}$，过拟合就不可避免

### 2.2 评估方法

选取尽可能与训练样本不同的测试集

#### 2.2.1 留出法

通常留出 1/5~1/3

#### 2.2.2 交叉验证

将样本划分为 K 个互斥子集

#### 2.2.3 自助法

给定包含 m 个样本的数据集 D，我们对他采样得到数据集 D'：每次从 D 中有放回的抽样，重复 m 次，用 D'作训练集，D - D'作测试集

- 在数据集小、难划分训练/测试集时很有用
- 但是改变了数据的分布，会引入估计偏差，数据够多一般不用此方法

#### 2.2.4 调参

略

### 2.3 性能度量

分类问题中常用的性能度量

#### 2.3.1 错误率与精度

二分类中常用，也可适用于多分类  
用$f$模型来衡量样例集$D$的错误率：

$$
E(f;D)=\frac{1}{m}\displaystyle\sum_{i=1}^{m}\mathbb{I}(f(x_i)\not ={y_i})
$$

精确度：

$$
acc(f;D)=1-E(f;D)
$$

更一般的，对数据分布${D}$和概率密度函数$p(\cdot)$，错误率：

$$
E(f;D)=\int_{x\sim D}\mathbb{I}(f(x)\not =y)\cdot p(x)\space dx
$$

#### 2.3.2 查准率、查全率与 F1

> precision/racall

**混淆矩阵：**

| 实际\预测      | 预测为真 | 预测为假 |
| -------------- | :------: | :------: |
| 为真(positive) |    TP    |    FN    |
| 为假(nagetive) |    FP    |    TN    |

**查准率 P**(挑出的瓜有多少是好的）和**查全率 R**（所有好瓜有多少被挑出来了）分别定义为：

$$
P=\frac{TP}{TP+FP}
$$

$$
R=\frac{TP}{TP+FN}
$$

度量模型好坏：F1 度量（更一般地，$F_\beta$度量）：

$$
\frac{1}{F_\beta}=\frac{1}{(1+\beta^2)}\times(\frac{1}{P}+\frac{\beta^2}{R})
$$

- $\beta$反映了对 P，R 重视程度
- F1 是$\beta=1$的情况
- 和算术平均/几何平均相比，调和平均更重视较小值

对于有多个混淆矩阵的情况（如多次测试/多个数据集上测试/多分类）有两种做法：

1. 各矩阵上分别计算 P，R，取平均，计算 F1（macro）
2. 平均各个混淆矩阵，再计算 P，R，F1（micro）

#### 2.3.3 ROC 与 AUC

> Receiver Operating Characteristic / Area Under Curve

![Receiver Operating Character](https://s1.ax1x.com/2020/05/12/YNFMh8.jpg)

反映的是『对结果预测置信度**排序能力**』的好坏：先把所有点设为 F（阈值设为最大，从(0,0)开始），从高到低，依次把点设为正例（逐步降低阈值），如果是真正例就向上走一步（让 AUC 变大更快），否则向右走一步  
loss = 1 - AUC

#### 2.3.4 代价敏感错误率与代价曲线\？?不太懂

对于不同类型错误造成的代价不一样的情况（如误判健康人为不健康/误判不健康为健康），可为错误赋予非均等代价

| 实际\预测 |   第 0 类   |   第 1 类   |
| --------- | :---------: | :---------: |
| 第 0 类   |      0      | $cost_{01}$ |
| 第 1 类   | $cost_{10}$ |      0      |

先前的性能度量大多隐式的假设均等代价，计算的是错误**次数**而非**总体代价**

![cost_curve](https://s1.ax1x.com/2020/05/12/YNFuAP.jpg)

### 2.4 比较检验

存在的问题：

1. 训练和测试的结果有差异
2. 结果与测试集的属性有很大联系
3. 随机性

解决问题的重要方法：**假设检验**  
通过统计来判断一个命题的真伪，需要量化一个评判标准$\alpha$  
可能的情况:

1. 标准太严，拒绝了本来为真的假设，记为$\alpha$，称之为显著水平(离谱水平)
2. 标准太松，接受了本来为假的假设。在一定的$\alpha$下我们希望受伪的概率越小越好。记受伪概率为$\pi=1-P(接受H_0|H_0为假)$，称之为检验的势。即在不冤枉好人的前提下最能发现坏人

![hypothesis_test](https://s1.ax1x.com/2020/05/12/YNFUA0.png)

> 记 training error 为$\hat{e}$，实际（泛化）错误率为$e$

#### 2.4.1 假设检验

在有 m 个样本的测试集上，泛化错误率为$e$的模型被测得错误率（training error）为$\hat{e}$的概率：

$$
P(\hat{e};e)=\left(^m _{\hat{e}\times m}\right)\cdot e^{\hat{e}\times m}\cdot (1-e)^{m-\hat{e}\times m}
$$

已知 training error 的情况下，解$\partial P(\hat{e};e)/\partial e=0$有，$P(\hat{e};e)$在$e=\hat{e}$时最大

假设$e\leqslant e_0$，则计算$\alpha$显著度下的临界$\bar{e}$，若测试错误率$\hat{e}$小于临界值$\bar{e}$，则可以得出结论：在$\alpha$显著度下不能拒绝$e\leqslant e_0$的假设，即能以$1-\alpha$的置信度认为泛化错误率不大于$e_0$

对于多次测试的情况，可以使用"t 检验"  
假定我们得到了 k 个错误率$\hat{e}_1,\hat{e}_2,...,\hat{e}_k$，则平均错误率$\mu$和方差$\sigma^2$为：

$$
\mu = \frac{1}{k}\displaystyle\sum_{i=1}^{k}\hat{e}_i
$$

$$
\sigma^2 = \frac{1}{k-1}\displaystyle\sum_{i=1}^k(\hat{e}_i-\mu)^2
$$

考虑这 k 个测试错误率可以看作泛化错误率$e_0$的独立采样，则变量

$$
\tau_t = \frac{\sqrt{k}(\mu-e_0)}{\sigma}
$$

服从自由的为 k-1 的 t 分布（详情见下文）。对假设"$\mu=e_0$"和显著度$\alpha$，我们可以计算在均值为$e_0$时，在$1-\alpha$概率内应该观察到的最大错误率（临界值），这里要考虑双边假设，两边“出界范围”都是$\alpha/2$，若平均错误率$|\mu-e_0| \in [t_{-\alpha/2},t_{\alpha/2}]$（在认为合理的面积内），则不能拒绝假设"$\mu=e_0$"，即有$1-\alpha$的置信度认为泛化错误率为$e_0$。
