# 周志华《机器学习》笔记(三)

> 关键词: AI, 读书笔记

## 3 线性模型

### 3.1 基本形式

给定由$d$个属性描述的示例$x = (x_1;x_2;...;x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型试图学得[ 通过属性的线性组合来进行预测 ]的函数，用向量的方式就写成

$$
f(x)=w^Tx+b
$$

### 3.2 线性回归

我们试图学得$f(x_i)=w^Tx_i+b,\space s.t.\space f(x_i)\simeq y_i$  
可利用最小二乘法对 w, b 进行估计。记$\hat{w}=(w;b)$，相应的，把数据集表示为一个$m\times (d+1)$的矩阵 X：

$$
X=
\left(
\begin{matrix}
x_1^T & 1\\
x_2^t & 1\\
\vdots & \vdots\\
x_m^T & 1
\end{matrix}
\right) \tag{1}
$$

再把标记也写成向量形式$y=(y_1;y_2;...;y_m)$，则有下式：

$$
\hat{w}^*=\argmin_{\hat{w}}(y-X\hat{w})^T(y-X\hat{w})
$$

其中$X\hat{w}$即预测的 y 值，上式即使偏差的平方和

令$E_{\hat{w}}=(y-X\hat{w})^T(y-X\hat{w})$，E 对$\hat{w}$求偏导：

$$
\frac{\partial E_{\hat{w}}}{\partial \hat{w}}=2X^T(X\hat{w}-y)
$$

解上式等于 0 即可得到 w  
?开始懵逼？  
当$X^TX$为满秩矩阵时或正定矩阵时

$$
\hat{w}^*=(X^TX)^{-1}X^Ty
$$

令$\hat{x}_i=(x_i,1)$,最终的模型为

$$
f(\hat{x}_i)=\hat{x}_i^T(X^TX)^{-1}X^Ty
$$

然而实际任务中$X^TX$往往不是满秩矩阵，此时可以解出多个$\hat{w}$，常见的做法是引入正则化  
?结束懵逼？  
更一般的，考虑单调可微函数$g(\cdot)$，令

$$
y = g^{-1}(w^Ts+b)
$$

这样得到的模型称为“广义线性模型”，其中$g(\cdot)$称为“联系函数”

### 3.3 对数几率回归（logistic regression）

对于分类问题，产生的结果$y\in \{0,1\}$，而线性回归产生的是实值，我们要将实值转换成 0/1 值，最理想的是分段函数，但是他并不连续，不可微，于是我们希望找到一定程度上近似的替代函数，通常用对数几率函数（logistic function）代替：

$$
y = \frac{1}{1+e^{-z}}\tag{*}
$$

变换后得：

$$
ln\frac{y}{1-y}=w^Tx+b\tag{\#}
$$

若将 y 视为产生正例的可能性，则 1-y 是产生反例的可能性，两者的比例称为“几率”（odds），反映了 x 作为正例的相对可能性，对几率取对数得到对数几率（log odds，亦称 logit）

由此可看出（\*）式实际上是在用线性模型的预测结果去逼近真实标记的对数几率，因此称这个模型为“对数几率回归”，他的优点有：

- 无需事先假设数据的分布
- 不仅能预测类别，还能得到近似的概率
- 对数几率函数任意阶可导

求解 w，b：将（#）式重写为：

$$
ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b
$$

显然有

$$
p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\tag{1}
$$

$$
p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}\tag{2}
$$

于是我们可以用“极大似然法”估计 w，b。对给定数据集，需要最大化如下函数（对数似然函数）：

$$
l(w,b)=\displaystyle\sum_{i=1}^mln\space p(y_i|x_i;w,b)\tag{3}
$$

为了便于讨论，令$\beta=(w;b),\hat{x}=(x;1)$，则$w^Tx+b$可简写为$\beta^T\hat{x}$，再令$p_1(\hat{x};\beta)=p(y=1|\hat{x};\beta)$, $p_0(\hat{x};\beta)=p(y=0|\hat{x};\beta)$，则似然函数可以重写为：

$$
p(y_i|x_i;w,b)=y_i\cdot p_1(\hat{x}_i;\beta)+(1-y_i)\cdot p_0(\hat{x}_i;\beta)
$$

结合上式和 123 式，最大化 3 式相当于最小化下式：

$$
l(\beta)=\displaystyle\sum_{i=1}^m(-y_i\beta^T\hat{x}_i+ln(1+e^{\beta^T\hat{x}}))
$$

证明如下：  
![maximum likelihood](https://s1.ax1x.com/2020/05/12/YNFm7t.jpg)  
$l(\beta)$是一个关于$\beta$的高阶可导连续凸函数，根据凸优化理论，可用牛顿法或者梯度下降迭代求解
